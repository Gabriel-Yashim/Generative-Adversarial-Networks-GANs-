{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgN9gZSp9riArzzedqQs73"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkaJ7qp2u3TR","executionInfo":{"status":"ok","timestamp":1666611817344,"user_tz":-60,"elapsed":66861,"user":{"displayName":"Gabriel Yashim","userId":"09182690880857804954"}},"outputId":"3d410cc6-4870-4d12-a8d7-115ce86f0387"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["import os\n","\n","os.chdir('/content/drive/My Drive/FY-PROJECT/implementation/mefash_dataset/image_segmentation/pytorch-nst-feedforward')"],"metadata":{"id":"zW4CXdgivZ4T","executionInfo":{"status":"ok","timestamp":1666612661434,"user_tz":-60,"elapsed":661,"user":{"displayName":"Gabriel Yashim","userId":"09182690880857804954"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["!pip install gitpython"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d_Z2SiF0s7S","executionInfo":{"status":"ok","timestamp":1666612143965,"user_tz":-60,"elapsed":6516,"user":{"displayName":"Gabriel Yashim","userId":"09182690880857804954"}},"outputId":"f19c41fe-9783-4c42-8c44-8c05c95942c9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gitpython\n","  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 16.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython) (4.1.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, gitdb, gitpython\n","Successfully installed gitdb-4.0.9 gitpython-3.1.29 smmap-5.0.0\n"]}]},{"cell_type":"code","source":["os.path.dirname(os.path.abspath(\"__file__\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oLHBN_LD9JVm","executionInfo":{"status":"ok","timestamp":1666614355816,"user_tz":-60,"elapsed":645,"user":{"displayName":"Gabriel Yashim","userId":"09182690880857804954"}},"outputId":"ada2fd68-c059-42e0-db4c-dd10805ba827"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/FY-PROJECT/implementation/mefash_dataset/image_segmentation/pytorch-nst-feedforward'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["import os\n","import argparse\n","import time\n","\n","import torch\n","from torch.optim import Adam\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","\n","from models.definitions.perceptual_loss_net import PerceptualLossNet\n","from models.definitions.transformer_net import TransformerNet\n","import utils.utils as utils\n","\n","\n","def train(training_config):\n","    # (tensorboard) writer will output to ./runs/ directory by default\n","    writer = SummaryWriter()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # prepare data loader\n","    train_loader = utils.get_training_data_loader(training_config)\n","\n","    # prepare neural networks\n","    transformer_net = TransformerNet().train().to(device)\n","    perceptual_loss_net = PerceptualLossNet(requires_grad=False).to(device)\n","\n","    optimizer = Adam(transformer_net.parameters())\n","\n","    # Calculate style image's Gram matrices (style representation)\n","    # Built over feature maps as produced by the perceptual net - VGG16\n","    style_images = '/content/drive/My Drive/FY-PROJECT/implementation/mefash_dataset/image_segmentation/pytorch-nst-feedforward/data/style-images'\n","    style_img_path = os.path.join(\n","        training_config['style_images_path'], training_config['style_img_name'])\n","    style_img = utils.prepare_img(\n","        style_img_path, target_shape=None, device=device, batch_size=training_config['batch_size'])\n","    style_img_set_of_feature_maps = perceptual_loss_net(style_img)\n","    target_style_representation = [utils.gram_matrix(\n","        x) for x in style_img_set_of_feature_maps]\n","\n","    utils.print_header(training_config)\n","    # Tracking loss metrics, NST is ill-posed we can only track loss and visual appearance of the stylized images\n","    acc_content_loss, acc_style_loss, acc_tv_loss = [0., 0., 0.]\n","    ts = time.time()\n","    for epoch in range(training_config['num_of_epochs']):\n","        for batch_id, (content_batch, _) in enumerate(train_loader):\n","            # step1: Feed content batch through transformer net\n","            content_batch = content_batch.to(device)\n","            stylized_batch = transformer_net(content_batch)\n","\n","            # step2: Feed content and stylized batch through perceptual net (VGG16)\n","            content_batch_set_of_feature_maps = perceptual_loss_net(\n","                content_batch)\n","            stylized_batch_set_of_feature_maps = perceptual_loss_net(\n","                stylized_batch)\n","\n","            # step3: Calculate content representations and content loss\n","            target_content_representation = content_batch_set_of_feature_maps.relu2_2\n","            current_content_representation = stylized_batch_set_of_feature_maps.relu2_2\n","            content_loss = training_config['content_weight'] * torch.nn.MSELoss(\n","                reduction='mean')(target_content_representation, current_content_representation)\n","\n","            # step4: Calculate style representation and style loss\n","            style_loss = 0.0\n","            current_style_representation = [utils.gram_matrix(\n","                x) for x in stylized_batch_set_of_feature_maps]\n","            for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):\n","                style_loss += torch.nn.MSELoss(\n","                    reduction='mean')(gram_gt, gram_hat)\n","            style_loss /= len(target_style_representation)\n","            style_loss *= training_config['style_weight']\n","\n","            # step5: Calculate total variation loss - enforces image smoothness\n","            tv_loss = training_config['tv_weight'] * \\\n","                utils.total_variation(stylized_batch)\n","\n","            # step6: Combine losses and do a backprop\n","            total_loss = content_loss + style_loss + tv_loss\n","            total_loss.backward()\n","            optimizer.step()\n","\n","            optimizer.zero_grad()  # clear gradients for the next round\n","\n","            #\n","            # Logging and checkpoint creation\n","            #\n","            acc_content_loss += content_loss.item()\n","            acc_style_loss += style_loss.item()\n","            acc_tv_loss += tv_loss.item()\n","\n","            if training_config['enable_tensorboard']:\n","                # log scalars\n","                writer.add_scalar('Loss/content-loss', content_loss.item(),\n","                                  len(train_loader) * epoch + batch_id + 1)\n","                writer.add_scalar('Loss/style-loss', style_loss.item(),\n","                                  len(train_loader) * epoch + batch_id + 1)\n","                writer.add_scalar('Loss/tv-loss', tv_loss.item(),\n","                                  len(train_loader) * epoch + batch_id + 1)\n","                writer.add_scalars('Statistics/min-max-mean-median', {'min': torch.min(stylized_batch), 'max': torch.max(\n","                    stylized_batch), 'mean': torch.mean(stylized_batch), 'median': torch.median(stylized_batch)}, len(train_loader) * epoch + batch_id + 1)\n","                # log stylized image\n","                if batch_id % training_config['image_log_freq'] == 0:\n","                    stylized = utils.post_process_image(\n","                        stylized_batch[0].detach().to('cpu').numpy())\n","                    # writer expects channel first image\n","                    stylized = np.moveaxis(stylized, 2, 0)\n","                    writer.add_image('stylized_img', stylized, len(\n","                        train_loader) * epoch + batch_id + 1)\n","\n","            if training_config['console_log_freq'] is not None and batch_id % training_config['console_log_freq'] == 0:\n","                print(f'time elapsed={(time.time()-ts)/60:.2f}[min]|epoch={epoch + 1}|batch=[{batch_id + 1}/{len(train_loader)}]|c-loss={acc_content_loss / training_config[\"console_log_freq\"]}|s-loss={acc_style_loss / training_config[\"console_log_freq\"]}|tv-loss={acc_tv_loss / training_config[\"console_log_freq\"]}|total loss={(acc_content_loss + acc_style_loss + acc_tv_loss) / training_config[\"console_log_freq\"]}')\n","                acc_content_loss, acc_style_loss, acc_tv_loss = [0., 0., 0.]\n","\n","            if training_config['checkpoint_freq'] is not None and (batch_id + 1) % training_config['checkpoint_freq'] == 0:\n","                training_state = utils.get_training_metadata(training_config)\n","                training_state[\"state_dict\"] = transformer_net.state_dict()\n","                training_state[\"optimizer_state\"] = optimizer.state_dict()\n","                ckpt_model_name = f\"ckpt_style_{training_config['style_img_name'].split('.')[0]}_cw_{str(training_config['content_weight'])}_sw_{str(training_config['style_weight'])}_tw_{str(training_config['tv_weight'])}_epoch_{epoch}_batch_{batch_id}.pth\"\n","                torch.save(training_state, os.path.join(\n","                    training_config['checkpoints_path'], ckpt_model_name))\n","\n","    #\n","    # Save model with additional metadata - like which commit was used to train the model, style/content weights, etc.\n","    #\n","    training_state = utils.get_training_metadata(training_config)\n","    training_state[\"state_dict\"] = transformer_net.state_dict()\n","    training_state[\"optimizer_state\"] = optimizer.state_dict()\n","    model_name = f\"style_{training_config['style_img_name'].split('.')[0]}_datapoints_{training_state['num_of_datapoints']}_cw_{str(training_config['content_weight'])}_sw_{str(training_config['style_weight'])}_tw_{str(training_config['tv_weight'])}.pth\"\n","    torch.save(training_state, os.path.join(\n","        training_config['model_binaries_path'], model_name))\n","\n","\n","if __name__ == \"__main__\":\n","    #\n","    # Fixed args - don't change these unless you have a good reason\n","    #\n","    dataset_path = os.path.join(os.path.dirname(\"__file__\"), 'data', 'mscoco')\n","    style_images_path = os.path.join(\n","        os.path.dirname(\"__file__\"), 'data', 'style-images')\n","    model_binaries_path = os.path.join(\n","        os.path.dirname(\"__file__\"), 'models', 'binaries')\n","    checkpoints_root_path = os.path.join(\n","        os.path.dirname(\"__file__\"), 'models', 'checkpoints')\n","    image_size = 256  # training images from MS COCO are resized to image_size x image_size\n","    batch_size = 4\n","\n","    assert os.path.exists(\n","        dataset_path), f'MS COCO missing. Download the dataset using resource_downloader.py script.'\n","    os.makedirs(model_binaries_path, exist_ok=True)\n","\n","    #\n","    # Modifiable args - feel free to play with these (only a small subset is exposed by design to avoid cluttering)\n","    #\n","    parser = argparse.ArgumentParser()\n","    # training related\n","    parser.add_argument(\"--style_img_name\", type=str,\n","                        help=\"style image name that will be used for training\", default='style1.jpg')\n","    parser.add_argument('-f')\n","    # you don't need to change this one just play with style loss\n","    parser.add_argument(\"--content_weight\", type=float,\n","                        help=\"weight factor for content loss\", default=1e0)\n","    parser.add_argument(\"--style_weight\", type=float,\n","                        help=\"weight factor for style loss\", default=4e5)\n","    parser.add_argument(\"--tv_weight\", type=float,\n","                        help=\"weight factor for total variation loss\", default=0)\n","    parser.add_argument(\"--num_of_epochs\", type=int,\n","                        help=\"number of training epochs \", default=2)\n","    parser.add_argument(\"--subset_size\", type=int,\n","                        help=\"number of MS COCO images (NOT BATCHES) to use, default is all (~83k)(specified by None)\", default=None)\n","    # logging/debugging/checkpoint related (helps a lot with experimentation)\n","    parser.add_argument(\"--enable_tensorboard\", type=bool,\n","                        help=\"enable tensorboard logging (scalars + images)\", default=True)\n","    parser.add_argument(\"--image_log_freq\", type=int,\n","                        help=\"tensorboard image logging (batch) frequency - enable_tensorboard must be True to use\", default=100)\n","    parser.add_argument(\"--console_log_freq\", type=int,\n","                        help=\"logging to output console (batch) frequency\", default=500)\n","    parser.add_argument(\"--checkpoint_freq\", type=int,\n","                        help=\"checkpoint model saving (batch) frequency\", default=2000)\n","    args = parser.parse_args()\n","\n","    checkpoints_path = os.path.join(\n","        checkpoints_root_path, args.style_img_name.split('.')[0])\n","    if args.checkpoint_freq is not None:\n","        os.makedirs(checkpoints_path, exist_ok=True)\n","\n","    # Wrapping training configuration into a dictionary\n","    training_config = dict()\n","    for arg in vars(args):\n","        training_config[arg] = getattr(args, arg)\n","    training_config['dataset_path'] = dataset_path\n","    training_config['style_images_path'] = style_images_path\n","    training_config['model_binaries_path'] = model_binaries_path\n","    training_config['checkpoints_path'] = checkpoints_path\n","    training_config['image_size'] = image_size\n","    training_config['batch_size'] = batch_size\n","\n","    # Original J.Johnson's training with improved transformer net architecture\n","    train(training_config)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5CmYAm6vZla","executionInfo":{"status":"error","timestamp":1666615755001,"user_tz":-60,"elapsed":1027353,"user":{"displayName":"Gabriel Yashim","userId":"09182690880857804954"}},"outputId":"ab835bbb-3f79-4a6f-cfab-4ec1b57ceb6e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Using 13880 datapoints (3470 batches) (MS COCO images) for transformer network training.\n","Learning the style of style1.jpg style image.\n","********************************************************************************\n","Hyperparams: content_weight=1.0, style_weight=400000.0 and tv_weight=0\n","********************************************************************************\n","Logging to console every 500 batches.\n","Saving checkpoint models every 2000 batches.\n","Tensorboard enabled.\n","Run \"tensorboard --logdir=runs --samples_per_plugin images=50\" from your conda env\n","Open http://localhost:6006/ in your browser and you're ready to use tensorboard!\n","********************************************************************************\n","time elapsed=0.01[min]|epoch=1|batch=[1/1735]|c-loss=0.012303243637084961|s-loss=1.0338896484375|tv-loss=0.0|total loss=1.0461928920745849\n","time elapsed=2.39[min]|epoch=1|batch=[501/1735]|c-loss=22.696433530807496|s-loss=40.36039933204651|tv-loss=0.0|total loss=63.056832862854\n","time elapsed=4.83[min]|epoch=1|batch=[1001/1735]|c-loss=23.346275199890137|s-loss=11.35726284790039|tv-loss=0.0|total loss=34.70353804779053\n","time elapsed=7.34[min]|epoch=1|batch=[1501/1735]|c-loss=23.206282402038575|s-loss=8.877982571601867|tv-loss=0.0|total loss=32.084264973640444\n","time elapsed=8.49[min]|epoch=2|batch=[1/1735]|c-loss=10.874724704742432|s-loss=3.2207994871139527|tv-loss=0.0|total loss=14.095524191856384\n","time elapsed=10.92[min]|epoch=2|batch=[501/1735]|c-loss=23.097969818115235|s-loss=6.750719761848449|tv-loss=0.0|total loss=29.848689579963683\n","time elapsed=13.39[min]|epoch=2|batch=[1001/1735]|c-loss=22.98621583557129|s-loss=6.370787034988403|tv-loss=0.0|total loss=29.357002870559693\n","time elapsed=15.92[min]|epoch=2|batch=[1501/1735]|c-loss=22.766176177978515|s-loss=5.414106079101563|tv-loss=0.0|total loss=28.180282257080076\n"]},{"output_type":"error","ename":"InvalidGitRepositoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidGitRepositoryError\u001b[0m                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-144ed0fefd9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Original J.Johnson's training with improved transformer net architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-144ed0fefd9d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(training_config)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Save model with additional metadata - like which commit was used to train the model, style/content weights, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtraining_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mtraining_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mtraining_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer_state\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/FY-PROJECT/implementation/mefash_dataset/image_segmentation/pytorch-nst-feedforward/utils/utils.py\u001b[0m in \u001b[0;36mget_training_metadata\u001b[0;34m(training_config)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mnum_of_datapoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subset_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_of_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     training_metadata = {\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;31m# \"commit_hash\": git.Repo(search_parent_directories=True).head.object.hexsha,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;34m\"content_weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;34m\"style_weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/git/repo/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, odbt, search_parent_directories, expand_vars)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# END working dir handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_working_tree_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGitCommandWrapperType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/git/repo/base.py\u001b[0m in \u001b[0;36mcommon_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# or could return \"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidGitRepositoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidGitRepositoryError\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"mxCT0x1uvZJn"},"execution_count":null,"outputs":[]}]}