# Deep-Convolutional-Generative-Adversarial-Networks-GANs-
## Computer Image Synthesis + Image Segmentation + Neural Style Transfer

<p>Generally, fashion is a way people express themselves through clothing, footwear, lifestyle, accessories, makeup, hairdo, and body posture in a specific time, place, and environment. The African fashion industry has continued to thrive over the years and has gained recognition globally although the fashion design and manufacturing process still follows the conventional methodology. Using deep learning techniques, a lot of research and studies have been done on image synthesis and the progress of the fashion sector with little or none of these research and techniques used on African Fashion.</p> 
<p>African fashion, especially Nigerian fashion lacks an online presence in terms of artificially generated realistic fashion styles. In this research work, we aim to develop a system that can generate new African fashion designs for men using <b>Deep Convolutional Generative Adversarial Networks (DCGAN)</b>. Two datasets were used to train the generator model of the DCGAN comparatively; a locally curated image data from publicly accessible and open-source images on the internet using web crawlers and custom python scripts, and the <b>AFRIFASHION40000 dataset</b>. The model trained with the AFRIFASHION40000 dataset was able to generate more realistic images than the locally curated data due to so much background noise on the locally curated data. The results from the model shows that DCGANs are unstable during the training process and are only good at generating low resolution images.</p>

<p>The system is an Afrocentric system that makes use of deep learning techniques (DCGAN) to generate new African fashion images. I designed a Male Fashion Generative Framework (MFGF) that is made up of a pair of neural networks and can generate new fashion images after being trained on image data. The model makes use of an optimization technique called style transfer. Style transfer is a technique used to blend two input images, --a subject or global image and a style image (the design or patch image) to produce a synthesized image that looks like or is based on the global image but has the design of the style image as output. This is achieved by adjusting the content parameters of the output image to suit the main image’s content parameter and the style image’s style parameter.</p>
<p>The MFGF is made up of two models, the pretrained DCGAN generator model, and the Neural Style Transfer model. The pretrained DCGAN generator model will take a set of random latent vectors (noise) as input with which it will use to generate a new fashion image. The generated image serves as input to the Neural Style Transfer model, which first performs image segmentation on the generated image to enable it apply the new style on some segments of the generated image. The styled image is returned as an output to the user.</p>

## Future Works
<p>DCGANs require a lot of computational resources and are quite unstable during training. DCGANs have the drawback of being only effective at producing little pictures (e.g., 64 x 64 pixels, 128 x 128 pixels). Future research should concentrate on creating new, higher-resolution (1024 x 1024 pixel) pictures using techniques like the Progressive Growing GAN method (Karras et al., 2017). Future research may concentrate on teaching the Neural Style Transfer model how to use spatial masks to apply styles to certain areas of the produced image.</p>
